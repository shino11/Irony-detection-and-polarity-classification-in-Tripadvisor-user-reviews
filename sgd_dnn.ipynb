{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jpDVYVg_RO8ZrjSIIz6vXsuiOyPQDzh9",
      "authorship_tag": "ABX9TyOtNCA/ZOc6tXbdQI8nNMlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shino11/Tesis_Deteccion-de-ironia-y-casificacion-de-Polaridad-en-rese-as-de-usuarios-de-Trip-Advisor/blob/main/sgd_dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Montar Drive"
      ],
      "metadata": {
        "id": "TfimqqnGDNMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3rJy6BnRWoY",
        "outputId": "c7135450-ad71-4fe6-f80b-110f74a63a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install & Verification"
      ],
      "metadata": {
        "id": "rwvXrdLtDTGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U tensorflow==2.2\n",
        "# !pip install -U numpy==1.19.5\n",
        "# !pip install h5py==2.10.0\n",
        "\n",
        "\n",
        "#!pip uninstall tensorflow\n",
        "#!pip install tensorflow\n",
        "#!pip uninstall numpy\n",
        "#!pip install numpy\n",
        "#!apt-get install python3.7\n",
        "\n",
        "#import sys\n",
        "#import tensorflow as tf\n",
        "\n",
        "\n",
        "#tf.__version__\n",
        "#!python3 ‐‐version\n",
        "#print(sys.version)\n",
        "\n",
        "!pip show scikit-learn pandas tensorflow numpy"
      ],
      "metadata": {
        "id": "rqS7jORZT7Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dnn_Algorithm_Part1"
      ],
      "metadata": {
        "id": "Mv7ozKCSETV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from builtins import print\n",
        "import numpy as np\n",
        "#import keras\n",
        "import tensorflow as tf\n",
        "#print('la version de tensorflow es:'+tf.__version__)\n",
        "import pandas as pd\n",
        "#from keras import Model\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "import os\n",
        "# from keras.engine.saving import model_from_json\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Flatten, Lambda\n",
        "# from keras.layers import LSTM, Input\n",
        "# from keras.layers import Dense, Embedding, Reshape\n",
        "# from keras.layers.wrappers import TimeDistributed\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "#from keras.utils.np_utils import to_categorical\n",
        "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score, balanced_accuracy_score, classification_report\n",
        "#from statsmodels.sandbox.distributions.examples.ex_mvelliptical import fig\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.python.keras.layers.core import Dropout, RepeatVector\n",
        "from tensorflow.keras.layers import LSTM, Reshape\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, LeakyReLU\n",
        "#from tensorflow.python.keras.engine.saving import model_from_json\n",
        "from tensorflow.python.training.adam import AdamOptimizer\n",
        "from tensorflow.keras import Input\n",
        "#from tensorflow_core.python.keras.layers import concatenate, Concatenate, Bidirectional\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.python.keras.models import load_model\n",
        "#import matplotlib #linux\n",
        "#matplotlib.use('Agg') #linux\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported).\n",
        " # The versions of TensorFlow you are currently using is 2.2.0 and is not supported.\n",
        "# import tensorflow_addons as tfa\n",
        "# import tqdm\n",
        "# # quietly deep-reload tqdm\n",
        "# import sys\n",
        "# from IPython.lib import deepreload\n",
        "# stdout = sys.stdout\n",
        "# sys.stdout = open('junk','w')\n",
        "# deepreload.reload(tqdm)\n",
        "# sys.stdout = stdout\n",
        "# tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
        "\n",
        "# import sys\n",
        "# sys.stdout = open(\"test.txt\", \"w\")\n",
        "\n",
        "#class HLSTM:\n",
        "#with tf.device('/gpu:0'):\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "#MAX_NB_WORDS = 100000\n",
        "text = []\n",
        "labels = []\n",
        "\n",
        "#activation='LeakyRelu'\n",
        "#activation='selu'\n",
        "activation='selu'\n",
        "#optimizer = 'Adadelta'\n",
        "optimizer = 'Adam'\n",
        "#learning_rate = 0.00001\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "#optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)\n",
        "dropout = 0.5\n",
        "neurons = 16\n",
        "patience = 10\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "folds = 5\n",
        "\n",
        "def save_model(model, filename):\n",
        "    model_json = model.to_json()\n",
        "    with open(filename + '.model', \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "        json_file.close();\n",
        "    model.save_weights(filename + \".weights\")\n",
        "\n",
        "\n",
        "# def load_model(filename):\n",
        "#     json_file = open(filename + '.model', 'r')\n",
        "#     loaded_model_json = json_file.read()\n",
        "#     json_file.close()\n",
        "#     # loaded_model = model_from_json(loaded_model_json)\n",
        "#     # loaded_model.load_weights(filename + \".weights\")\n",
        "#     # return loaded_model;\n",
        "\n",
        "\n",
        "def getEmbeddingLayer():\n",
        "    embeddings_index = {}\n",
        "    countt = 0\n",
        "    words = []\n",
        "    #f = open('data/word_embeddings/glove.840B.300d.txt', encoding='utf-8', errors='ignore')\n",
        "    #f = open('data/word_embeddings/glove.6B.100d.txt', encoding='utf-8')\n",
        "    f = open('/content/drive/MyDrive/Colab Notebooks/TestCode/glove.6B.100d.txt')\n",
        "    #f = open('word_embeddings/glove.6B.100d.txt')\n",
        "\n",
        "    for line in f:\n",
        "        values = line.split()  #espacio para glove 840b only\n",
        "        word = values[0]\n",
        "        words.append(word)\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "        countt = countt + 1;\n",
        "    f.close()\n",
        "\n",
        "    # fin = io.open('data/word_embeddings/GoogleNews-vectors-negative300.bin', 'r', encoding='utf-8', newline='\\n',\n",
        "    #               errors='ignore')\n",
        "    # n, d = map(int, fin.readline().split())\n",
        "    # for line in fin:\n",
        "    #     tokens = line.rstrip().split(' ')\n",
        "    #     word = tokens[0]\n",
        "    #     words.append(word)\n",
        "    #     embeddings_index[tokens[0]] = map(float, tokens[1:])\n",
        "    #     count += 1\n",
        "    # fin.close()\n",
        "\n",
        "    #tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer = Tokenizer(num_words=len(words)+1)\n",
        "    tokenizer.fit_on_texts(words)\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index) + 1\n",
        "\n",
        "    print(\"total words embeddings is \", countt, len(word_index))\n",
        "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        # if i > vocab_size - 1:\n",
        "        #     break\n",
        "        # else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                                output_dim=EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "\n",
        "    return tokenizer, embedding_layer\n",
        "\n",
        "\n",
        "# seed = 7346\n",
        "# np.random.seed(seed)\n",
        "count = 0\n",
        "precision_list = list()\n",
        "recall_list = list()\n",
        "accuracy_list = list()\n",
        "f1_list = list()\n",
        "\n",
        "\n",
        "def create_embedded_model(embedding_layer, num_classes, MAX_SEQUENCE_LENGTH):\n",
        "    input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "    emb_layer = embedding_layer(input)\n",
        "    #model.add(tf.keras.layers.Dense(32, kernel_initializer='lecun_normal', activation='selu'))\n",
        "    out = Flatten()(emb_layer)\n",
        "    #out = Dense(32, kernel_initializer='lecun_normal', activation='selu')(out)\n",
        "    out = Dense(neurons, kernel_initializer='lecun_normal', activation=activation)(out)\n",
        "    #out = Dense(neurons, kernel_initializer='lecun_normal')(out)\n",
        "    #out = LeakyReLU()(out)\n",
        "    out = Dropout(rate=dropout)(out)\n",
        "    output = Dense(2, activation='sigmoid')(out)\n",
        "    modell = Model(input, output)\n",
        "    print(modell.summary())\n",
        "    return modell;\n",
        "\n",
        "\n",
        "# Start here\n",
        "\n",
        "dataframe = pd.DataFrame()\n",
        "#dataframe = pd.read_csv(\"./data/sgd single-domains/sgd_act_Flights.csv\", encoding='ISO-8859-1', sep='^', engine='python')\n",
        "#dataframe = pd.read_csv(\"./data/sgd single-domains/sgd_act_Alarm.csv\", encoding='ISO-8859-1', sep='^', engine='python')\n",
        "#dataframe = pd.read_csv(\"data/sgd single-domains/sgd-act-prec1-Flights.csv\", encoding='ISO-8859-1', sep='^', engine='python')\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/sarcasm_v2(0-1).csv\", encoding='ISO-8859-1', sep=',', engine='python')\n",
        "#dataframe = pd.read_csv(\"sarcasm_v2(0-1).csv\", encoding='ISO-8859-1', sep=',', engine='python')\n",
        "\n",
        "print(dataframe.head())\n",
        "# dataframe = dataframe.replace(to_replace=r'\\s,,\\s', value=' ')\n",
        "print(dataframe.size)\n",
        "\n",
        "# cols = ['dialogue','text','label']\n",
        "# cols = ['dialogue', 'text', 'prec_utt', 'prec_intent', 'label']\n",
        "cols = ['Corpus', 'Label', 'ID', 'Quote Text', 'Response Text']\n",
        "\n",
        "dataframe.columns = cols\n",
        "#D = dataframe.loc[:, 'dialogue']\n",
        "X = dataframe.loc[:, 'Response Text']\n",
        "# A = dataframe.loc[:, 'prec_utt']\n",
        "B = dataframe.loc[:, 'Quote Text']\n",
        "Y = dataframe.loc[:, 'Label']\n",
        "print(dataframe.shape)\n",
        "#X = X.reshape((len(X),1))\n",
        "\n",
        "X_featured = []\n",
        "for i in range(X.count()):\n",
        "    X_featured.append(X[i] + ' ' + B[i])\n",
        "\t  #X_featured.append(X[i] + ' ' + A[i] + ' ' + B[i])\n",
        "X_featured = np.reshape(X_featured, X.shape)\n",
        "#print(X_featured.shape)\n",
        "#print(dataframe.shape)\n",
        "# X = X.reshape((len(X),1))\n",
        "print(X.count())\n",
        "print(Y.count())\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in X])\n",
        "tokenizer, embedding = getEmbeddingLayer()\n",
        "\n",
        "sequences_x = tokenizer.texts_to_sequences(X_featured) #X_featured\n",
        "# sequences_a = tokenizer.texts_to_sequences(A)\n",
        "# sequences_b = tokenizer.texts_to_sequences(B)\n",
        "#sequences_a2 = tokenizer.texts_to_sequences(A2)\n",
        "#sequences_b2 = tokenizer.texts_to_sequences(B2)\n",
        "\n",
        "#dd = pad_sequences(sequences_d, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "xx = pad_sequences(sequences_x, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating=\"post\")\n",
        "# aa = pad_sequences(sequences_a, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# bb = pad_sequences(sequences_b, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "#aa2 = pad_sequences(sequences_a2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "#bb2 = pad_sequences(sequences_b2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# d = np.transpose(d)\n",
        "data = xx\n",
        "#print(data)\n",
        "uniques, ids = np.unique(Y, return_inverse=True)\n",
        "y_train = tf.keras.utils.to_categorical(ids, len(uniques))\n",
        "# y_train = np.reshape(y_train, (Y.count(),len(uniques)))\n",
        "# y_train = keras.utils.to_categorical(Y,Y.count())\n",
        "#print(len(uniques))\n",
        "#print(y_train)\n",
        "# y_train = np.array(y_train.shape[1])\n",
        "# df = pd.DataFrame(data=y_train.data)\n",
        "# y_train = np.asarray(Y)\n",
        "\n"
      ],
      "metadata": {
        "id": "HHLAP8SWgRS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a725d4-cd8a-4658-ab16-cf71eb5ca4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Corpus  Label             ID  \\\n",
            "0    GEN      1  GEN_sarc_0000   \n",
            "1    GEN      1  GEN_sarc_0001   \n",
            "2    GEN      1  GEN_sarc_0002   \n",
            "3    GEN      1  GEN_sarc_0003   \n",
            "4    GEN      1  GEN_sarc_0004   \n",
            "\n",
            "                                          Quote Text  \\\n",
            "0  First off, That's grade A USDA approved Libera...   \n",
            "1  watch it. Now you're using my lines. Poet has ...   \n",
            "2  Because it will encourage teens to engage in r...   \n",
            "3  Obviously you missed the point. So sorry the t...   \n",
            "4  This is pure paranoia. What evidence do you ha...   \n",
            "\n",
            "                                       Response Text  \n",
            "0  Therefore you accept that the Republican party...  \n",
            "1  More chattering from the peanut gallery? Haven...  \n",
            "2  Yep, suppressing natural behavior is always th...  \n",
            "3  I guess we all missed your point Justine, what...  \n",
            "4  Evidence, I dont need no sticking evidence. Th...  \n",
            "23460\n",
            "(4692, 5)\n",
            "4692\n",
            "4692\n",
            "total words embeddings is  400000 339251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DNN_Algorithm_Part2"
      ],
      "metadata": {
        "id": "RRDxfRMqEdku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data\")\n",
        "print(data.shape, y_train.shape)\n",
        "\n",
        "# np.random.seed(1)\n",
        "# y = np.random.randint(0, len(num_classes)-1, y_train.shape)\n",
        "# y = y[np.where(y.sum(axis=1) != 0)[0]]\n",
        "# y_new = LabelEncoder().fit_transform([''.join(str(l)) for l in y])\n",
        "# print(y_new)\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "historyl = list()\n",
        "# cross validation\n",
        "kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "#optimizer = AdamOptimizer(learning_rate=0.0001)\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "for train, test in kfold.split(data, Y):\n",
        "    # x_train, x_test, Y_train, Y_test = train_test_split(data, y_train, test_size=0.8, random_state=0)\n",
        "    count += 1\n",
        "    print('Fold: {0}'.format(count))\n",
        "    print(data[train].shape, y_train[train].shape)\n",
        "    model = create_embedded_model(embedding, len(uniques), MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    # adad = tf.train.AdadeltaOptimizer()\n",
        "    # adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    print('Training...')\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
        "    mc = ModelCheckpoint('best_model'+str(count)+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "    # model.fit(np.reshape(data[train],(-1,data[train].shape[0],data[train].shape[1])), y_train[train], epochs=50, batch_size=batch_size, verbose=1, steps_per_epoch=20)  # 50 , steps_per_epoch=20\n",
        "    history = model.fit(data[train], y_train[train], validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[es, mc])  # 50 , steps_per_epoch=20\n",
        "    historyl.append(history)\n",
        "    # save_model(model, \"data/model\")\n",
        "    saved_model = load_model('best_model'+str(count)+'.h5')\n",
        "    # HASTA AQUÃ  OK------------------------------------------------------------\n",
        "\n",
        "    # TESTING\n",
        "    print()\n",
        "    print(\"Testing data\")\n",
        "    print(data[test].shape, y_train[test].shape)\n",
        "\n",
        "    score, acc = saved_model.evaluate(data[test], y_train[test], verbose=1)\n",
        "    print(\"acc: \", acc)\n",
        "    print(\"Accuracy: {0:.2%}\".format(acc))\n",
        "    print('Test score:', score)\n",
        "    y_predict = saved_model.predict(data[test])\n",
        "    # y_predict = np.argmax(y_predict, axis=1)\n",
        "    print(y_predict.shape)\n",
        "    labels_pred = np.argmax(np.round(y_predict),axis=1)\n",
        "    labels_test = np.argmax(y_train[test], axis=1)\n",
        "    print(labels_test)\n",
        "    print(labels_pred)\n",
        "    #print(y_train[test], np.round(y_predict))\n",
        "    accur = accuracy_score(y_train[test], np.round(y_predict))\n",
        "    # print(roc_auc_score(Y[test], y_predict, average='macro'))\n",
        "    print('Test accuracy:', accur)\n",
        "    #print(confusion_matrix(np.argmax(y_train[test],axis=1), np.round(y_predict).argmax(axis=1)))\n",
        "    b_acc = balanced_accuracy_score(labels_test, labels_pred)\n",
        "    recall = recall_score(labels_test, labels_pred, average='weighted')\n",
        "    precision = precision_score(labels_test, labels_pred, average='weighted', labels=np.unique(labels_pred))\n",
        "    f1 = f1_score(labels_test, labels_pred, average='weighted')\n",
        "    print('Test b_acc:', b_acc)\n",
        "    print('Test precision:', precision)\n",
        "    print('Test recall:', recall)\n",
        "    print('Test f1:', f1)\n",
        "    accuracy_list.append(b_acc)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "    print(classification_report(labels_test, labels_pred))\n",
        "    # print(history.history.keys())\n",
        "    plt.plot(history.history['loss'], color='blue')\n",
        "    plt.plot(history.history['val_loss'], color='red')\n",
        "    # plt.title(str(count))\n",
        "    #plt.show()\n",
        "    # fig.savefig(str(count)+'.png', bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','validation'], loc='upper left')\n",
        "# plt.show()\n",
        "# plt.boxplot(accuracy_list)\n",
        "plt.show()\n",
        "# fig.savefig('box.png', bbox_inches='tight', pad_inches=0)\n",
        "print('avg b_accuracy precision recall f1')\n",
        "# print('avg b_accuracy {0}'.format(np.average(accuracy_list)))\n",
        "# print('avg precision {0}'.format(np.average(precision_list)))\n",
        "# print('avg recall {0}'.format(np.average(recall_list)))\n",
        "# print('avg f1 {0}'.format(np.average(f1_list)))\n",
        "print(np.average(accuracy_list))\n",
        "print(np.average(precision_list))\n",
        "print(np.average(recall_list))\n",
        "f1_final = np.average(f1_list)\n",
        "print(f1_final)\n",
        "\n",
        "fig.savefig('loss_dnn(patience='+str(patience)+', epochs='+str(epochs)+', batch_size='+str(batch_size)+', neurons= '+str(neurons)+', activation='+str(activation)+', Dropout='+str(dropout)+', fold='+str(folds)+', optimizer='+str(optimizer)+', f1=' + str(f1_final) +').png', bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "#plt.figure()\n",
        "# sys.stdout.close()"
      ],
      "metadata": {
        "id": "dw6Yn0_ArEDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}