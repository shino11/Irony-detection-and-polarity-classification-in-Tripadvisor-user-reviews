{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jpDVYVg_RO8ZrjSIIz6vXsuiOyPQDzh9",
      "authorship_tag": "ABX9TyO92Fr/6ohQBzRfD/FEA8f5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shino11/Tesis_Deteccion-de-ironia-y-casificacion-de-Polaridad-en-rese-as-de-usuarios-de-Trip-Advisor/blob/main/sgd_dnn(bert).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Montar Drive"
      ],
      "metadata": {
        "id": "TfimqqnGDNMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3rJy6BnRWoY",
        "outputId": "c7135450-ad71-4fe6-f80b-110f74a63a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install & Verification"
      ],
      "metadata": {
        "id": "rwvXrdLtDTGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U tensorflow==2.2\n",
        "!pip install -U numpy==1.19.5\n",
        "!pip install h5py==2.10.0\n",
        "#!pip install tensorflow-text --no-dependencies\n",
        "!pip install transformers\n",
        "!pip install tensorflow==2.10\n",
        "#!pip install keras==2.4.3\n",
        "#!pip install -U numpy\n",
        "#!pip install tensorflow-gpu\n",
        "\n",
        "#!pip uninstall tensorflow\n",
        "#!pip install tensorflow\n",
        "#!pip uninstall numpy\n",
        "#!pip install numpy\n",
        "#!apt-get install python3.7\n",
        "\n",
        "#import sys\n",
        "#import tensorflow as tf\n",
        "\n",
        "\n",
        "#tf.__version__\n",
        "#!python3 ‐‐version\n",
        "#print(sys.version)\n",
        "\n",
        "#Egr.576 (pass_HPC)\n",
        "\n",
        "!pip show numpy tensorflow"
      ],
      "metadata": {
        "id": "rqS7jORZT7Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dnn_Algorithm_Part1"
      ],
      "metadata": {
        "id": "Mv7ozKCSETV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from builtins import print\n",
        "import numpy as np\n",
        "#import keras\n",
        "import tensorflow as tf\n",
        "#print('la version de tensorflow es:'+tf.__version__)\n",
        "import pandas as pd\n",
        "#from keras import Model\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "import os\n",
        "# from keras.engine.saving import model_from_json\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Flatten, Lambda\n",
        "# from keras.layers import LSTM, Input\n",
        "# from keras.layers import Dense, Embedding, Reshape\n",
        "# from keras.layers.wrappers import TimeDistributed\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "#from keras.utils.np_utils import to_categorical\n",
        "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score, balanced_accuracy_score, classification_report\n",
        "#from statsmodels.sandbox.distributions.examples.ex_mvelliptical import fig\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.python.keras.layers.core import Dropout, RepeatVector\n",
        "from tensorflow.keras.layers import LSTM, Reshape\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, LeakyReLU\n",
        "#from tensorflow.python.keras.engine.saving import model_from_json\n",
        "from tensorflow.python.training.adam import AdamOptimizer\n",
        "from tensorflow.keras import Input\n",
        "#from tensorflow_core.python.keras.layers import concatenate, Concatenate, Bidirectional\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.python.keras.models import load_model\n",
        "#import matplotlib #linux\n",
        "#matplotlib.use('Agg') #linux\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported).\n",
        " # The versions of TensorFlow you are currently using is 2.2.0 and is not supported.\n",
        "# import tensorflow_addons as tfa\n",
        "# import tqdm\n",
        "# # quietly deep-reload tqdm\n",
        "# import sys\n",
        "# from IPython.lib import deepreload\n",
        "# stdout = sys.stdout\n",
        "# sys.stdout = open('junk','w')\n",
        "# deepreload.reload(tqdm)\n",
        "# sys.stdout = stdout\n",
        "# tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
        "\n",
        "# import sys\n",
        "# sys.stdout = open(\"test.txt\", \"w\")\n",
        "\n",
        "#class HLSTM:\n",
        "#with tf.device('/gpu:0'):\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "#MAX_NB_WORDS = 100000\n",
        "text = []\n",
        "labels = []\n",
        "\n",
        "#activation='LeakyRelu'\n",
        "#activation='selu'\n",
        "activation='selu'\n",
        "#optimizer = 'Adadelta'\n",
        "optimizer = 'Adam'\n",
        "#learning_rate = 0.00001\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "#optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)\n",
        "dropout = 0.5\n",
        "neurons = 16\n",
        "patience = 10\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "folds = 5\n",
        "\n",
        "def save_model(model, filename):\n",
        "    model_json = model.to_json()\n",
        "    with open(filename + '.model', \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "        json_file.close();\n",
        "    model.save_weights(filename + \".weights\")\n",
        "\n",
        "\n",
        "# def load_model(filename):\n",
        "#     json_file = open(filename + '.model', 'r')\n",
        "#     loaded_model_json = json_file.read()\n",
        "#     json_file.close()\n",
        "#     # loaded_model = model_from_json(loaded_model_json)\n",
        "#     # loaded_model.load_weights(filename + \".weights\")\n",
        "#     # return loaded_model;\n",
        "\n",
        "\n",
        "def getEmbeddingLayer():\n",
        "    embeddings_index = {}\n",
        "    countt = 0\n",
        "    words = []\n",
        "    #f = open('data/word_embeddings/glove.840B.300d.txt', encoding='utf-8', errors='ignore')\n",
        "    #f = open('data/word_embeddings/glove.6B.100d.txt', encoding='utf-8')\n",
        "    f = open('/content/drive/MyDrive/Colab Notebooks/TestCode/glove.6B.100d.txt')\n",
        "    #f = open('word_embeddings/glove.6B.100d.txt')\n",
        "\n",
        "    for line in f:\n",
        "        values = line.split()  #espacio para glove 840b only\n",
        "        word = values[0]\n",
        "        words.append(word)\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "        countt = countt + 1;\n",
        "    f.close()\n",
        "\n",
        "    # fin = io.open('data/word_embeddings/GoogleNews-vectors-negative300.bin', 'r', encoding='utf-8', newline='\\n',\n",
        "    #               errors='ignore')\n",
        "    # n, d = map(int, fin.readline().split())\n",
        "    # for line in fin:\n",
        "    #     tokens = line.rstrip().split(' ')\n",
        "    #     word = tokens[0]\n",
        "    #     words.append(word)\n",
        "    #     embeddings_index[tokens[0]] = map(float, tokens[1:])\n",
        "    #     count += 1\n",
        "    # fin.close()\n",
        "\n",
        "    #tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer = Tokenizer(num_words=len(words)+1)\n",
        "    tokenizer.fit_on_texts(words)\n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size = len(word_index) + 1\n",
        "\n",
        "    print(\"total words embeddings is \", countt, len(word_index))\n",
        "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        # if i > vocab_size - 1:\n",
        "        #     break\n",
        "        # else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                                output_dim=EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "\n",
        "    return tokenizer, embedding_layer\n",
        "\n",
        "\n",
        "# seed = 7346\n",
        "# np.random.seed(seed)\n",
        "count = 0\n",
        "precision_list = list()\n",
        "recall_list = list()\n",
        "accuracy_list = list()\n",
        "f1_list = list()\n",
        "\n",
        "\n",
        "def create_embedded_model(embedding_layer, num_classes, MAX_SEQUENCE_LENGTH):\n",
        "    input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "    emb_layer = embedding_layer(input)\n",
        "    #model.add(tf.keras.layers.Dense(32, kernel_initializer='lecun_normal', activation='selu'))\n",
        "    out = Flatten()(emb_layer)\n",
        "    #out = Dense(32, kernel_initializer='lecun_normal', activation='selu')(out)\n",
        "    out = Dense(neurons, kernel_initializer='lecun_normal', activation=activation)(out)\n",
        "    #out = Dense(neurons, kernel_initializer='lecun_normal')(out)\n",
        "    #out = LeakyReLU()(out)\n",
        "    out = Dropout(rate=dropout)(out)\n",
        "    output = Dense(2, activation='sigmoid')(out)\n",
        "    modell = Model(input, output)\n",
        "    print(modell.summary())\n",
        "    return modell;\n",
        "\n",
        "\n",
        "# Start here\n",
        "\n",
        "dataframe = pd.DataFrame()\n",
        "#dataframe = pd.read_csv(\"./data/sgd single-domains/sgd_act_Flights.csv\", encoding='ISO-8859-1', sep='^', engine='python')\n",
        "#dataframe = pd.read_csv(\"./data/sgd single-domains/sgd_act_Alarm.csv\", encoding='ISO-8859-1', sep='^', engine='python')\n",
        "#dataframe = pd.read_csv(\"data/sgd single-domains/sgd-act-prec1-Flights.csv\", encoding='ISO-8859-1', sep='^', engine='python')\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/sarcasm_v2(0-1).csv\", encoding='ISO-8859-1', sep=',', engine='python')\n",
        "#dataframe = pd.read_csv(\"sarcasm_v2(0-1).csv\", encoding='ISO-8859-1', sep=',', engine='python')\n",
        "\n",
        "print(dataframe.head())\n",
        "# dataframe = dataframe.replace(to_replace=r'\\s,,\\s', value=' ')\n",
        "print(dataframe.size)\n",
        "\n",
        "# cols = ['dialogue','text','label']\n",
        "# cols = ['dialogue', 'text', 'prec_utt', 'prec_intent', 'label']\n",
        "cols = ['Corpus', 'Label', 'ID', 'Quote Text', 'Response Text']\n",
        "\n",
        "dataframe.columns = cols\n",
        "#D = dataframe.loc[:, 'dialogue']\n",
        "X = dataframe.loc[:, 'Response Text']\n",
        "# A = dataframe.loc[:, 'prec_utt']\n",
        "B = dataframe.loc[:, 'Quote Text']\n",
        "Y = dataframe.loc[:, 'Label']\n",
        "print(dataframe.shape)\n",
        "#X = X.reshape((len(X),1))\n",
        "\n",
        "X_featured = []\n",
        "for i in range(X.count()):\n",
        "    X_featured.append(X[i] + ' ' + B[i])\n",
        "\t  #X_featured.append(X[i] + ' ' + A[i] + ' ' + B[i])\n",
        "X_featured = np.reshape(X_featured, X.shape)\n",
        "#print(X_featured.shape)\n",
        "#print(dataframe.shape)\n",
        "# X = X.reshape((len(X),1))\n",
        "print(X.count())\n",
        "print(Y.count())\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in X])\n",
        "tokenizer, embedding = getEmbeddingLayer()\n",
        "\n",
        "sequences_x = tokenizer.texts_to_sequences(X_featured) #X_featured\n",
        "# sequences_a = tokenizer.texts_to_sequences(A)\n",
        "# sequences_b = tokenizer.texts_to_sequences(B)\n",
        "#sequences_a2 = tokenizer.texts_to_sequences(A2)\n",
        "#sequences_b2 = tokenizer.texts_to_sequences(B2)\n",
        "\n",
        "#dd = pad_sequences(sequences_d, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "xx = pad_sequences(sequences_x, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating=\"post\")\n",
        "# aa = pad_sequences(sequences_a, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "# bb = pad_sequences(sequences_b, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "#aa2 = pad_sequences(sequences_a2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "#bb2 = pad_sequences(sequences_b2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# d = np.transpose(d)\n",
        "data = xx\n",
        "#print(data)\n",
        "uniques, ids = np.unique(Y, return_inverse=True)\n",
        "y_train = tf.keras.utils.to_categorical(ids, len(uniques))\n",
        "# y_train = np.reshape(y_train, (Y.count(),len(uniques)))\n",
        "# y_train = keras.utils.to_categorical(Y,Y.count())\n",
        "#print(len(uniques))\n",
        "#print(y_train)\n",
        "# y_train = np.array(y_train.shape[1])\n",
        "# df = pd.DataFrame(data=y_train.data)\n",
        "# y_train = np.asarray(Y)\n",
        "\n"
      ],
      "metadata": {
        "id": "HHLAP8SWgRS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a725d4-cd8a-4658-ab16-cf71eb5ca4db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Corpus  Label             ID  \\\n",
            "0    GEN      1  GEN_sarc_0000   \n",
            "1    GEN      1  GEN_sarc_0001   \n",
            "2    GEN      1  GEN_sarc_0002   \n",
            "3    GEN      1  GEN_sarc_0003   \n",
            "4    GEN      1  GEN_sarc_0004   \n",
            "\n",
            "                                          Quote Text  \\\n",
            "0  First off, That's grade A USDA approved Libera...   \n",
            "1  watch it. Now you're using my lines. Poet has ...   \n",
            "2  Because it will encourage teens to engage in r...   \n",
            "3  Obviously you missed the point. So sorry the t...   \n",
            "4  This is pure paranoia. What evidence do you ha...   \n",
            "\n",
            "                                       Response Text  \n",
            "0  Therefore you accept that the Republican party...  \n",
            "1  More chattering from the peanut gallery? Haven...  \n",
            "2  Yep, suppressing natural behavior is always th...  \n",
            "3  I guess we all missed your point Justine, what...  \n",
            "4  Evidence, I dont need no sticking evidence. Th...  \n",
            "23460\n",
            "(4692, 5)\n",
            "4692\n",
            "4692\n",
            "total words embeddings is  400000 339251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DNN_Algorithm_Part2"
      ],
      "metadata": {
        "id": "RRDxfRMqEdku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data\")\n",
        "print(data.shape, y_train.shape)\n",
        "\n",
        "# np.random.seed(1)\n",
        "# y = np.random.randint(0, len(num_classes)-1, y_train.shape)\n",
        "# y = y[np.where(y.sum(axis=1) != 0)[0]]\n",
        "# y_new = LabelEncoder().fit_transform([''.join(str(l)) for l in y])\n",
        "# print(y_new)\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "historyl = list()\n",
        "# cross validation\n",
        "kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "#optimizer = AdamOptimizer(learning_rate=0.0001)\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "for train, test in kfold.split(data, Y):\n",
        "    # x_train, x_test, Y_train, Y_test = train_test_split(data, y_train, test_size=0.8, random_state=0)\n",
        "    count += 1\n",
        "    print('Fold: {0}'.format(count))\n",
        "    print(data[train].shape, y_train[train].shape)\n",
        "    model = create_embedded_model(embedding, len(uniques), MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    # adad = tf.train.AdadeltaOptimizer()\n",
        "    # adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    print('Training...')\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
        "    mc = ModelCheckpoint('best_model'+str(count)+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "    # model.fit(np.reshape(data[train],(-1,data[train].shape[0],data[train].shape[1])), y_train[train], epochs=50, batch_size=batch_size, verbose=1, steps_per_epoch=20)  # 50 , steps_per_epoch=20\n",
        "    history = model.fit(data[train], y_train[train], validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[es, mc])  # 50 , steps_per_epoch=20\n",
        "    historyl.append(history)\n",
        "    # save_model(model, \"data/model\")\n",
        "    saved_model = load_model('best_model'+str(count)+'.h5')\n",
        "    # HASTA AQUÃ  OK------------------------------------------------------------\n",
        "\n",
        "    # TESTING\n",
        "    print()\n",
        "    print(\"Testing data\")\n",
        "    print(data[test].shape, y_train[test].shape)\n",
        "\n",
        "    score, acc = saved_model.evaluate(data[test], y_train[test], verbose=1)\n",
        "    print(\"acc: \", acc)\n",
        "    print(\"Accuracy: {0:.2%}\".format(acc))\n",
        "    print('Test score:', score)\n",
        "    y_predict = saved_model.predict(data[test])\n",
        "    # y_predict = np.argmax(y_predict, axis=1)\n",
        "    print(y_predict.shape)\n",
        "    labels_pred = np.argmax(np.round(y_predict),axis=1)\n",
        "    labels_test = np.argmax(y_train[test], axis=1)\n",
        "    print(labels_test)\n",
        "    print(labels_pred)\n",
        "    #print(y_train[test], np.round(y_predict))\n",
        "    accur = accuracy_score(y_train[test], np.round(y_predict))\n",
        "    # print(roc_auc_score(Y[test], y_predict, average='macro'))\n",
        "    print('Test accuracy:', accur)\n",
        "    #print(confusion_matrix(np.argmax(y_train[test],axis=1), np.round(y_predict).argmax(axis=1)))\n",
        "    b_acc = balanced_accuracy_score(labels_test, labels_pred)\n",
        "    recall = recall_score(labels_test, labels_pred, average='weighted')\n",
        "    precision = precision_score(labels_test, labels_pred, average='weighted', labels=np.unique(labels_pred))\n",
        "    f1 = f1_score(labels_test, labels_pred, average='weighted')\n",
        "    print('Test b_acc:', b_acc)\n",
        "    print('Test precision:', precision)\n",
        "    print('Test recall:', recall)\n",
        "    print('Test f1:', f1)\n",
        "    accuracy_list.append(b_acc)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "    print(classification_report(labels_test, labels_pred))\n",
        "    # print(history.history.keys())\n",
        "    plt.plot(history.history['loss'], color='blue')\n",
        "    plt.plot(history.history['val_loss'], color='red')\n",
        "    # plt.title(str(count))\n",
        "    #plt.show()\n",
        "    # fig.savefig(str(count)+'.png', bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','validation'], loc='upper left')\n",
        "# plt.show()\n",
        "# plt.boxplot(accuracy_list)\n",
        "plt.show()\n",
        "# fig.savefig('box.png', bbox_inches='tight', pad_inches=0)\n",
        "print('avg b_accuracy precision recall f1')\n",
        "# print('avg b_accuracy {0}'.format(np.average(accuracy_list)))\n",
        "# print('avg precision {0}'.format(np.average(precision_list)))\n",
        "# print('avg recall {0}'.format(np.average(recall_list)))\n",
        "# print('avg f1 {0}'.format(np.average(f1_list)))\n",
        "print(np.average(accuracy_list))\n",
        "print(np.average(precision_list))\n",
        "print(np.average(recall_list))\n",
        "f1_final = np.average(f1_list)\n",
        "print(f1_final)\n",
        "\n",
        "fig.savefig('loss_dnn(patience='+str(patience)+', epochs='+str(epochs)+', batch_size='+str(batch_size)+', neurons= '+str(neurons)+', activation='+str(activation)+', Dropout='+str(dropout)+', fold='+str(folds)+', optimizer='+str(optimizer)+', f1=' + str(f1_final) +').png', bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "#plt.figure()\n",
        "# sys.stdout.close()"
      ],
      "metadata": {
        "id": "dw6Yn0_ArEDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "yY3vbBQeDl3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sarcasm_v2(0-1).csv')\n",
        "#datastore = df.drop(['Corpus', 'Label', 'ID', 'Quote Text', 'Response Text'], axis=1)\n",
        "#print(datastore)\n",
        "\n",
        "#shuffle data to split into train, test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_featured, Y, train_size = 0.8, random_state = 0, shuffle = True)\n",
        "print(X_featured.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-10T18:20:58.446243Z",
          "iopub.execute_input": "2021-07-10T18:20:58.446932Z",
          "iopub.status.idle": "2021-07-10T18:20:58.812447Z",
          "shell.execute_reply.started": "2021-07-10T18:20:58.446881Z",
          "shell.execute_reply": "2021-07-10T18:20:58.811721Z"
        },
        "trusted": true,
        "id": "XkDPTb7thdmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc63636-5307-4dd6-efd4-3854048b3256"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4692,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT\n",
        "from transformers import BertTokenizer\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "\n",
        "def input_for_bert_model(x_list, tokenizer, max_seq_length, state):\n",
        "    #columns_list = dataframe.columns.tolist()\n",
        "    input_ids = np.zeros((len(x_list), MAX_SEQUENCE_LENGTH))\n",
        "    input_attention_masks = np.zeros((len(x_list), MAX_SEQUENCE_LENGTH))\n",
        "    \n",
        "    if state:\n",
        "        train_labels = np.zeros((len(Y_train), 1))\n",
        "        print(train_labels.shape)\n",
        "        for i, label in enumerate(Y_train):\n",
        "        #for i in range(len(Y_train)):\n",
        "            # print(i)\n",
        "            # label = x_list[i]\n",
        "            train_labels[i,:] = int(label)\n",
        "    \n",
        "    for i, sequence in enumerate(x_list):\n",
        "        tokens = tokenizer.encode_plus(\n",
        "            sequence,\n",
        "            max_length = max_seq_length, # max length of the text that can go to BERT\n",
        "            truncation=True, padding='max_length',\n",
        "            add_special_tokens = True, # add [CLS], [SEP]\n",
        "            return_token_type_ids = False, \n",
        "            return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "            return_tensors = 'tf'\n",
        "        )\n",
        "        input_ids[i,:], input_attention_masks[i,:] = tokens['input_ids'], tokens['attention_mask']\n",
        "    \n",
        "    if state:\n",
        "        return input_ids, input_attention_masks, train_labels\n",
        "    else:\n",
        "        return input_ids, input_attention_masks\n",
        "    \n",
        "\n",
        "\n",
        "#print(\"train dataframe \\n\",train_dataframe, \"bert tokenizer\\n\",bert_tokenizer, \"max length\\n\",max_length)\n",
        "train_ids, train_attention_masks, train_labels = input_for_bert_model(X_train, bert_tokenizer, MAX_SEQUENCE_LENGTH, True)\n",
        "\n",
        "test_ids, test_attention_masks = input_for_bert_model(X_test, bert_tokenizer, MAX_SEQUENCE_LENGTH, False)\n",
        "\n",
        "train_inputs = {\"input_ids\":train_ids[:len(X_train)], \"attention_mask\":train_attention_masks[:len(X_train)]}\n",
        "print(X_train.shape)\n",
        "train_outputs = train_labels[:len(Y_train)]\n",
        "print(Y_train.shape)\n",
        "valid_inputs = {\"input_ids\":train_ids[len(X_test):], \"attention_mask\":train_attention_masks[len(X_test):]}\n",
        "print(X_test.shape)\n",
        "valid_outputs = train_labels[len(Y_test):]\n",
        "print(Y_test.shape)\n",
        "test_inputs = {\"input_ids\":test_ids, \"attention_mask\":test_attention_masks}\n",
        "# test_outputs = test_labels[4000:]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-10T18:21:40.52969Z",
          "iopub.execute_input": "2021-07-10T18:21:40.530131Z",
          "iopub.status.idle": "2021-07-10T18:21:57.757714Z",
          "shell.execute_reply.started": "2021-07-10T18:21:40.530097Z",
          "shell.execute_reply": "2021-07-10T18:21:57.756932Z"
        },
        "trusted": true,
        "id": "BrF3483Fhdmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d603155f-c3de-4de1-8513-6ca716230a32"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3753, 1)\n",
            "(3753,)\n",
            "(3753,)\n",
            "(939,)\n",
            "(939,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert model initialization\n",
        "from transformers import BertTokenizer\n",
        "#from tensorflow.python.keras.saving.hdf5_format import save_attributes_to_hdf5_group\n",
        "from transformers import TFBertModel\n",
        "\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype=tf.int32, name=\"attention_mask\")\n",
        "sequence_output = bert_model(input_ids, attention_mask=attention_mask)[0][:,0,:]\n",
        "\n",
        "out = tf.keras.layers.Dropout(0.2)(sequence_output)\n",
        "out = Flatten()(out)\n",
        "out = tf.keras.layers.Dense(1, activation='sigmoid', name=\"outputs\")(out)\n",
        "model4 = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=out)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-10T18:22:38.412584Z",
          "iopub.execute_input": "2021-07-10T18:22:38.413065Z",
          "iopub.status.idle": "2021-07-10T18:22:47.482556Z",
          "shell.execute_reply.started": "2021-07-10T18:22:38.413035Z",
          "shell.execute_reply": "2021-07-10T18:22:47.481765Z"
        },
        "trusted": true,
        "id": "5w65jsB7hdmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb2b707-f71f-468f-d35a-204dd5d668bf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model4.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history4 = model4.fit(train_inputs, train_outputs, epochs=10, batch_size=64, validation_data=(valid_inputs, valid_outputs), verbose=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-10T18:23:40.664264Z",
          "iopub.execute_input": "2021-07-10T18:23:40.664619Z"
        },
        "trusted": true,
        "id": "Pt35KC7-hdms",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "outputId": "ab6160e4-8ee0-474f-d899-8f3f2cd09675"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-5d78b5b641a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1053, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5678, in binary_crossentropy\n        labels=target, logits=output\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n"
          ]
        }
      ]
    }
  ]
}